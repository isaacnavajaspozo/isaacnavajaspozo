# Proxmox VE (PVE) ¬∑ Documentaci√≥n 10/07/2025
=======[‚ùå TEORIA ]=================================================================================
[üßÆ Arquitectura Interna Proxmox]:
# KVM sirve para virtualizar m√°quinas completas en Linux, permiti√©ndote ejecutar m√∫ltiples sistemas operativos (VMs) aislados sobre un mismo host f√≠sico, Proxmox utiliza el KVM que ya viene en el kernel de Linux, integr√°ndolo con:
#     - QEMU (para emulaci√≥n de hardware "vm")
#     - LXC (para contenedores)
#     - libvirt y sus propias APIs para gesti√≥n
#     - Su propia interfaz web y CLI (pve)

+-------------------+  +-------------+      
|  Virtual Machine  |  |  CONTAINER  |      # KVM (Kernel-based Virtual Machine): Es capaz de crear m√°quinas virtuales utilizando al KVM[QUEMU] del Linux principal (Hipervisor) 
|        KVM        |  |     LXC     |      # LXC (Linux Containers): Es capaz de crear contenedores utilizando al KVM[LXC] del Linux principal (aunque en realidad no utiliza el hipervisor, si no que es virtualizaci√≥n a nivel de sistema operativo, aprobechando el mismo kernel) 
+-------------------+  +-------------+
          |                   |
+------------------------------------+
|             PROXMOX VE             |
+------------------------------------+
                  |
+------------------------------------+      # Como Proxmox es una virtualizaci√≥n de tipo 1 el hipervisor y el S.O forman parte del mismo. En las virtualizaciones tipo 2 se separa el S.O del Hipervisor
|             HIPERVISOR             |      # Sistema principal de KVM del servidor Linux Kernel, del que va a utilizar Proxmox para virtualizar sus m√°quinas y crear los contenedores
+------------------------------------+      # El Hipervisor no es una parte separator√≠a si no que es el propio kernel del Linux de Proxmox, pero lo granulo para comprender mejor cada parte
+------------------------------------+      # El hipervisor utiliza el LXC y el KVM integrado en el propio Linux Kernel para luego desde Proxmox crear las m√°quinas virtuales y los contenedores
|               LINUX                |      # Distribuci√≥n/Linux por defecto para Proxmox: Debian
+------------------------------------+
                  |
+------------------------------------+      # Storage (ZFS, LVM, Ceph, ext4, etc)
|              HARDWARE              |      #   + Espacio asignado por disco virtual a cada VM/CT.
+------------------------------------+      #   + Snapshots, backups y thin provisioning seg√∫n el tipo de almacenamiento.
+---------------+  +-----------------+      # Gesti√≥n de recursos
|    STORAGE    |  |     NETWORK     |      #   + CPU: Puedes asignar n√∫cleos virtuales (vCPU). Proxmox permite overcommit.
+---------------+  +-----------------+      #   + RAM: Limitada, puedes usar ballooning para ajustar din√°micamente.
                                            #   + Disco: Asignado como almacenamiento virtual (raw, qcow2, LVM, ZVOL).
                                            #   + Red: Interfaces virtuales puenteadas al host (bridge).                      

[üßÆ Fundamento t√©cnico discos]:
- Thinpool : Permitir asignar m√°s almacenamiento virtual del que realmente tienes disponible f√≠sicamente.

[üßÆ Fundamento t√©cnico vCPU]:
- Overcommit: Overcommit (o sobre-asignaci√≥n) se refiere a la pr√°ctica de reservar m√°s recursos virtuales (CPU, memoria, disco, etc.) de los que f√≠sicamente hay disponibles en el host. El overcommit de CPU es posible porque Proxmox/KVM permite asignar m√°s vCPUs que cores f√≠sicos.
- Time-slicing: El scheduler (programador de procesos) reparte el tiempo de cada core f√≠sico en peque√±as rebanadas (‚Äútime slices‚Äù). Cada vCPU de cada VM entra en cola y recibe su rebanada cuando le toque turno, hasta que se agoten o pase el slice al siguiente hilo.
- Linux KVM/QEMU y el scheduler (programador de procesos) del host manejan la distribuci√≥n del tiempo de CPU entre las VMs, gestionando el acceso concurrente.
- Cores asignados en Proxmox: pertenece a vCPUs (virtual CPU) que configuras para cada VM (m√°quina virtual), "esto quiere decir que no pertenece a los cores f√≠sicos reales si no a los virtuales".

[üßÆ Fundamento t√©cnico Cluster] HA (pol√≠ticas de alta disponibilidad):
# Un cluster es cuando tienes 2 o m√°s nodos Proxmox trabajando juntos, compartiendo gesti√≥n de recursos, VMs, etc.

- Nodo: va a ser cada uno de los espacios dentro del ‚ÄúDatacenter‚Äù y reprensenta cada hardware (los servidores)

- split-brain: Es cuando un cluster se divide por un fallo de red, y dos partes creen que son la v√°lidas, actuando al mismo tiempo. Esto puede provocar: Corrupci√≥n de discos compartidos, Dos VMs corriendo con el mismo ID (una por nodo) e Inconsistencias.

- Quorum: En Proxmox VE (que utiliza Corosync como capa de clustering), el ‚Äúquorum‚Äù es el mecanismo que asegura que s√≥lo un subconjunto mayoritario de nodos pueda tomar decisiones v√°lidas (como iniciar/migrar/apagar) sobre el estado del cl√∫ster y sus recursos. El quorum es un mecanismo de seguridad que garantiza que el cluster tiene suficientes nodos comunic√°ndose entre s√≠ para operar con seguridad (sin perdida de datos). Sirve para evitar particiones cerebrales o split-brain. El quorum por simplificarlo sirve para saber que nodo esta activo o desactivado, funciona por votos y los dispositivos tienen que ser de n√∫mero impar. El quorum no va a permitir habilitar nuevos servicios o nuevas m√°quinas etc si el quorum esta desabilitado, en el caso de que el quorum este habilitado todo seguir√° funcionando correctamente. Como el comando "pvecm" epertenece al cluster est√° directamente relacionado con el quorum.
    Se basa en el n√∫mero de nodos activos y visibles entre s√≠:
      quorum = (n√∫mero de nodos / 2) + 1
    * Ejemplo Cluster de 3 nodos: Quorum m√≠nimo: 2 nodos activos y conectados
    * Ejemplo Cluster de 4 nodos: Quorum m√≠nimo: 3 nodos activos
    * Ejemplo Si no hay quorum: el nodo bloquea operaciones de gesti√≥n (crear/mover VMs, etc.)
    ‚ùó¬øPor qu√© mejor n√∫mero impar en quorum?
    Con n√∫mero impar: Siempre hay una mayor√≠a clara y Menos probabilidad de empates
    Con n√∫mero par: Si la red se parte 2 y 2 ‚Üí ning√∫n grupo tiene mayor√≠a y Resultado: todo se bloquea

- QDevice: Se instala en una m√°quina f√≠sica (con bajos recursos ya que solo corre un servicio llamado corosync-qnetd) cuando los nodos actuales son pares (ya que para que funcione el quorum tienen que ser impares). Es un tercer votante neutral de voto (solo vota), que no ejecuta VMs, pero sirve para desempatar y ayudar al quorum (se a√±ade a un cluster Proxmox con nodos pares para ayudar a mantener quorum).
    Es √∫til cuando: Tienes solo 2 nodos f√≠sicos y No puedes a√±adir un tercer nodo completo
    Es ligero, solo vota en el cluster y Se configura con corosync + qnetd
    * Ejemplo Tienes 2 nodos (node1 y node2): Si node1 y node2 pierden conexi√≥n entre ellos ‚Üí El qdevice vota por el que siga conectado a √©l y Solo ese nodo mantiene quorum, el otro se a√≠sla.


[üßÆ Forma t√©cnica y directa seg√∫n c√≥mo Proxmox gestiona RAM y CPU]:
# esto es una estimaci√≥n peor siempre puede ser relativo seg√∫n necesidades
üß† RAM ‚Äì Repartici√≥n f√≠sica (sin overcommit):
- La RAM se reserva de forma exclusiva para cada VM/CT.
- Si tienes 4GB RAM f√≠sicas y restas ~1GB para el sistema (Proxmox + servicios), te quedan 3GB √∫tiles. Si cada VM tiebe 1GB de RAM puedo crear hasta 3 VMs.
üß© No se recomienda consumir el 100% de la RAM disponible. Siempre deja margen para el sistema (~1GB m√≠nimo en tu caso).
+--------------------+----------------------+------------------------+
| RAM disponible     | RAM por VM/CT        | M√°quinas posibles      |
+--------------------+----------------------+------------------------+
| 4GB (total)        | 1GB sistema host      | ~3GB √∫tiles           |
| ~3GB para VMs/CTs  | 1GB por VM            | Hasta 3 VMs           |
| ~3GB para VMs/CTs  | 512MB por VM          | Hasta 6 VMs           |
| ~3GB para VMs/CTs  | 256MB por CT (LXC)    | Hasta 10‚Äì11 CTs       |
+--------------------+----------------------+------------------------+

‚öôÔ∏è CPU ‚Äì Asignaci√≥n l√≥gica (overcommit permitido):
- Las vCPU son virtuales, puedes tener m√°s vCPUs que cores f√≠sicos.
- Generalmente se usa overcommit: por ejemplo, 4 cores f√≠sicos pueden soportar 6‚Äì8 vCPUs o m√°s.
- Lo importante es la carga real de uso.
‚ö†Ô∏è Estoy repartiendo tiempo de CPU, no creando potencia adicional real.  
üß† Overcommit con prudencia: evalaci√≥n seg√∫n la carga real que cada VM va a generar y los picos de ejecuci√≥n en tiempos asignados, no solo el n√∫mero de vCPUs.
+----------------+-----------------------------------+------------------------+
| Cores f√≠sicos  | VMs ligeras posibles (con 1 vCPU) | Recomendaci√≥n          |
+----------------+-----------------------------------+------------------------+
| 4              | 6‚Äì8 VMs (light use) o 12‚Äì20 LXCs  | OK con bajo uso        |
| 4              | 3‚Äì4 VMs (alta carga) o 8‚Äì10 LXCs  | Mejor rendimiento      |
+----------------+-----------------------------------+------------------------+

M√©todo                         | ¬øCPU f√≠sica directa?         | Descripci√≥n t√©cnica
-------------------------------|------------------------------|-------------------------------------------------------------
Asignar vCPU (por defecto )    | ‚ùå No                       | Lo m√°s com√∫n. El hipervisor distribuye el uso entre vCPUs.
CPU pinning                    | ‚úÖ Parcialmente             | Asocia vCPUs a cores f√≠sicos espec√≠ficos. Ejemplos: taskset, numactl, configuraci√≥n en XML/QEMU args.
CPU passthrough                | ‚úÖ S√≠, pero para PCI        | Passthrough para hardware PCI (GPU, etc.). No aplica para CPU.

-------------------------------------------------------------------------------------
Consejos t√©cnicos:
- Usa drivers **virtio** para disco y red, mejora rendimiento y compatibilidad.
- Habilita **ballooning** para RAM en laboratorios para mejor gesti√≥n din√°mica.
- Para menos overhead, considera usar contenedores (LXC) en vez de VMs.
- Aplica **CPU limits y CPU shares** para controlar recursos si varias VMs se activan a la vez.
-------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------
‚ùó Ejemplo con Hardware (1TB HDD, 4GB RAM, 4 CPU):
Supuestos:
- Sistema base Proxmox requiere ~512MB ‚Äì 1GB RAM y ~10-20GB de disco.
- Asignamos 3GB RAM y ~950GB para VMs/CTs.
- CPU puede sobreprovisionarse (ej: 6-8 vCPUs repartidas en 4 cores f√≠sicos).

+----------------+---------------+------------+----------------+--------------------+
| Tipo           | RAM Asignada  | CPU (vCPU) | Disco Asignado | M√°quinas posibles  |
+----------------+---------------+------------+----------------+--------------------+
| VM ligera      | 512MB         | 1 vCPU     | 10GB           | 5‚Äì6 VMs            |
| LXC b√°sico     | 256MB         | 0.5 vCPU   | 2GB            | 8‚Äì10 CTs           |
| VM media       | 1GB           | 1 vCPU     | 20GB           | 2‚Äì3 VMs            |
| Mezcla VM+CT   | VMs: 2√ó1GB    | 2√ó1 vCPU   | 40GB total     | + 4 CTs con 512MB  |
+----------------+---------------+------------+----------------+--------------------+

üß† Recomendaciones:
Usa LXC para servicios Linux siempre que puedas ‚Üí mayor densidad.
Activa RAM ballooning en VMs si el SO lo soporta.
Usa ZFS si necesitas snapshots/flexibilidad, pero requiere m√°s RAM.
Mant√©n siempre RAM libre para el sistema (~1GB m√≠nimo).
Si vas a tener muchas VMs, considera ampliar la RAM del ejemplo.
-------------------------------------------------------------------------------------



=======[‚ùå PASOS A SEGUIR DE INSTALACI√ìN ]==========================================================
# sigo los pasos seg√∫n la instalaci√≥n requerida:

          - Crear unidad ISO: Instalaci√≥n de ISO Proxmox VE
          - Configuraci√≥n de red:
          - Configuraci√≥n de discos y unidades: (unidad ISOs)
          - Configuraci√≥n de scripts: Scripts de la comunidad (Post, hardening...)
          - Crear cluster : En el caso de ser necesario HA ‚Üí configuro y creo cluster
                    + Configurar nodo como cluster (NTP activo y en hora)
                    + Configurar Recursos Comunes del Cluster



=======[‚ùå CREAR UNIDAD ISO ]=======================================================================
# descargar iso
https://www.proxmox.com/en/downloads

## acceso proxmox instalaci√≥n
Proxmox Virtual Environment : https://www.proxmox.com/en/downloads [Proxmox VE 8.4 ISO Installer]
- Disco en ext4 (el espacio es para la instalaci√≥n de proxmox)
- zona horaria
- <password-root>
- ip
- DNS Server: 255.255.255.0
htts://<ip>:8006



=======[‚ùå CONFIGURACI√ìN DE RED E INSTALACI√ìN M√çNIMA ]==============================================
# comprobar que la red est√° bien configurada
vim /etc/network/interfaces

# instalaci√≥n m√≠nima
apt update && apt upgrade
apt install btop vim -y

## SSH
# entro por ssh y modifico la terminal
vim ~/.bashrc
--------------------------------------------------------------
## alias del servidor
alias ls='ls -ha --color=auto --group-directories-first'
alias la='ls  -lhai --group-directories-first'
alias _liberarespacioram='sudo sync; echo 1 | sudo tee /proc/sys/vm/drop_caches | echo "pe    tici√≥n realizada correctamente." && echo "" && free -h'
alias cp='cp -i'
alias mv='mv -i'
alias rm='rm -i'
alias grep='grep --color=auto'
alias df='df --exclude-type=tmpfs'

## Cambiar dise√±o del prompt (estilo cyberpunk)
# **************************************
PS1='\[\e[1;31m\]\uÁÆ±[\H] \w $: \[\e[0m\]'

## cambiar colores para ls (estilo cyberpunk)
# **************************************
# Estilo t√©cnico oscuro
LS_COLORS=""
LS_COLORS+="di=1;38;5;240:"     # directorios en gris oscuro t√©cnico
LS_COLORS+="fi=0;38;5;248:"     # archivos normales en gris medio
LS_COLORS+="ln=1;33:"           # enlaces simb√≥licos en amarillo
LS_COLORS+="so=1;38;5;94:"      # sockets en p√∫rpura oscuro
LS_COLORS+="pi=1;33:"           # pipes (tuber√≠as) en amarillo
LS_COLORS+="bd=1;38;5;236:"     # dispositivos de bloque en gris carb√≥n
LS_COLORS+="cd=1;38;5;238:"     # dispositivos de car√°cter en gris plomo
LS_COLORS+="or=1;38;5;124:"     # archivos rotos en rojo sangre oscuro
LS_COLORS+="mi=1;38;5;124:"     # archivos inexistentes en rojo sangre oscuro
LS_COLORS+="ex=1;31"            # ejecutables en rojo brillante
--------------------------------------------------------------

# modifico el archivo vim
cd /
vim .vimrc
--------------------------------------------------------------
" ~/.vimrc simplificado estilo cyberpunk acorde a LS_COLORS
set number                                    " Muestra n√∫meros de l√≠nea
set cursorline                                " Resalta l√≠nea actual
set scrolloff=5                               " Mantiene 5 l√≠neas arriba/abajo
set incsearch                                 " B√∫squeda incremental
set hlsearch                                  " Resalta resultados b√∫squeda
set ignorecase                                " Ignora may√∫sculas/min√∫sculas
set smartcase                                 " Caso sensible si hay may√∫sculas
set expandtab                                 " Usa espacios en lugar de tabs
set tabstop=4
set shiftwidth=4
set wildmenu                                  " Mejor autocompletado en cmd
syntax on
set background=dark

" Colores personalizados b√°sicos para consola
highlight Normal ctermfg=248 ctermbg=236      " Texto gris medio / fondo gris carb√≥n
highlight Comment ctermfg=33                  " Comentarios azul marino fuerte
highlight LineNr ctermfg=240                  " N√∫meros gris oscuro
highlight CursorLine ctermbg=238              " Fondo l√≠nea cursor gris plomo
highlight Keyword ctermfg=61                  " Palabras clave azul cobalto apagado
highlight Function ctermfg=94                 " Funciones p√∫rpura oscuro
highlight Statement ctermfg=124               " Sentencias rojo sangre oscuro
highlight Visual ctermbg=240                  " Selecci√≥n gris oscuro

set laststatus=2                              " Siempre mostrar l√≠nea de estado
set noerrorbells                              " Sin sonidos de error
set clipboard=unnamedplus                     " Usa portapapeles del sistema
--------------------------------------------------------------



=======[‚ùå CONFIGURACI√ìN DE DISCOS Y UNIDADES ]=====================================================
# primero creo el Thinpool y lo a√±ado como almacenamiento
Thinpool : Permitir asignar m√°s almacenamiento virtual del que realmente tienes disponible f√≠sicamente.

# creo la partici√≥n mayor en modo LVM-Thin
Nodo > Disk > LVM-Thin > Create Thinpool 
  > [ ] add storage    # si lo seleccionas se agregar√° autom√°ticamente al almacenamiento

# crea un almacenamiento (para asociar el tipo de almacenamiento y agregarlo al nodo)
Datacenter > Storage > Add > LVM-Thin > Selecciona "nombre-disco"
# desde aqu√≠ configuro el tipo de unidad que voy a crear pudiendo ser por ejemplo en Directory (Disk Image, ISO Image, Container, Backup...)
# seg√∫n las opciones al clicar en el nodo > storage me dar√° unas selecciones u otras

+--------------+-----------------------------+----------------------------------------+
| Tipo         | Uso adecuado                | Contenido a seleccionar (‚ÄúContent‚Äù)    |
+--------------+-----------------------------+----------------------------------------+
| LVM-Thin     | Discos para VM/CT           | Disk image, Container                  |
| LVM          | Solo discos de VM           | Disk image                             |
| Directory    | Ficheros: ISOs, backups...  | ISO image, Backup, Container, Disk img |
| ZFS          | Similar a LVM-Thin + snaps  | Disk image, Container, ISO*            |
| NFS / CIFS   | Almacenamiento en red       | ISO, Backup, Container, Disk image     |
| Ceph         | Cluster distribuido         | Disk image, Container                  |
+--------------+-----------------------------+----------------------------------------+
* ZFS solo permite ISO si se monta como dataset compatible


## en el caso de necesitar m√°s discos f√≠sicos 
----------------------------------------------------------
# 1. (Opcional) Verifica y limpia particiones del disco nuevo
lsblk                     # Verifica el disco, por ejemplo /dev/sdb
fdisk /dev/sdb
  - d                     # Elimina particiones si existen
  - n                     # Crear nueva partici√≥n (por defecto, usa todo el disco). Hay que hacerlo en el caso que se requieran particiones del disco, en el caso de que solo sea una no es necesario ya que vamos a crear un thinpool
  - w                     # Guarda y sale

# 2. (Opcional) Verifica estado del disco [Verifica salud SMART]
# S.M.A.R.T = sistema integrado en discos duros y SSDs que permite auto-monitorear su salud y detectar fallos potenciales antes de que ocurran
smartctl -iHA /dev/sdb

# 3. Crear Thinpool desde Proxmox
# Thinpool : Permitir asignar m√°s almacenamiento virtual del que realmente tienes disponible f√≠sicamente.
GUI: Node > Disks > LVM-Thin > Create: 
  - Select disk: /dev/sdb
  - Name: thinpool-disk2
  - Opcional: eliminar particiones existentes (GUI lo hace)
  - Esto crea autom√°ticamente PV + VG + Thin Pool

# 4. A√±adir Thinpool como almacenamiento
GUI: Datacenter > Storage > Add > LVM-Thin
  - ID: lvm-disk2
  - Volume Group: el creado (vg-thinpool-disk2, por ejemplo)
  - Thin Pool: thinpool-disk2
  - Content: Disk image, Container

# 5. (Opcional) Verifica LVM
pvdisplay
vgdisplay
lvdisplay
----------------------------------------------------------


## yo en mi caso en local (pve) que es mi unidad de almacenamiento de proxmox no quiero que contenga m√°s de 20GB y por defecto le d√° 100GB 

# 1. Comprueba el tama√±o actual
lvdisplay /dev/pve/data                   # compruebo el almacenamiento (90GB)
lvs                                       # Vista resumida

# 2. Verifica si hay VMs o CTs en ejecuci√≥n
qm list                                   # M√°quinas virtuales
pct list                                  # Contenedores

# 3. Aseg√∫rate de que 'local-lvm' no tenga discos asignados
ls /dev/pve/                              # Revisa qu√© vol√∫menes hay
ls /var/lib/lvm                           # Si usas LVM directamente
du -sh /dev/pve/data                      # Verifica espacio usado

# ‚ö†Ô∏è Aseg√∫rate que est√© vac√≠o o solo ocupado parcialmente

# 4. Reducci√≥n del volumen
lvresize -L 20G /dev/pve/data
# o:
lvreduce -L 20G /dev/pve/data             # Interactivo y m√°s seguro

# 5. Verifica
lvs



=======[‚ùå CONFIGURACI√ìN DE SCRIPTS ]===============================================================
# cambio el tema
usuario (arriba a la derecha) > Color Theme > Proxmox Dark

# instalamos el script inicializador
https://community-scripts.github.io/ProxmoxVE/scripts?id=post-pve-install
(proxmox:selecciono el nodo) > Shell > (copio y pego el script)
- y /all

# instalo script para cambiar a powersave (elige como el CPU escala su frecuencia y consumo de los watts)
https://community-scripts.github.io/ProxmoxVE/scripts?id=scaling-governor
[*] powersave
- y

# instalo script para ver si el procesador tiene alguna actualizaci√≥n
https://community-scripts.github.io/ProxmoxVE/scripts?id=microcode
[*] intel-microcode_3.20240514.1_amd64.deb
- y

# ‚ö†Ô∏è de este √∫ltimo script no se si fiarme ya que es de alguien independiente y no oficial de proxmox, lo quiero dejar documentado por si ag√∫n d√≠a es √∫til para m√≠
# instalo script proxmenux para tareas de administrador de servidor (drivers, m√°quinas virtuales...)
# https://github.com/MacRimi/ProxMenux
# en la pagina princial, donde muestra el readme solo tengo que copiar y pegar el instalador : bash -c "$(wget -qLO - https://raw.githubusercontent.com/MacRimi/ProxMenux/main/install_proxmenux.sh)"
# - normal
# - y
#
# para abrirlo "menu"
# ahora desde proxmox puedo isntalar paquetes de proxmox sin tener que salir de proxmox, ahora en este ejemplo voy a instalar adguards sin salir de proxmox
# > menu
#   > Proxmox Helpers Script
#   > search : adguards
#   > yes
#   > Desafult Settings

# reinicio el servidor proxmox
- reiniciar



=======[‚ùå CREAR CLUSTER ]==========================================================================
# antes de iniciar con los cluster hay que intentar que los dispositivos o servidores sean los m√°s parecido posible a ser posible exactamente iguales

[üßÆ creo un cluster]::
# Para migrar un nodo, el nodo que vamos a migrar como cluster no debe tener ninguna m√°quina virtual ni contenedor y adem√°s que contenga el mismo NTP "timedatectl status"
# una vez tenga el nodo limpio me dirijo al otro nodo (al que vamos a migrar el nuevo nodo) y hago lo siguiente:
# ejemplo proxmox pve1
-----------------------------------------------------------------
Datacenter > Cluster (pve1) > Create Cluster
          Cluster Demo: <nombre>
          Cluster Network: <asigno_la_ip_dentro_del_cluster>
          > create
# una vez creado
Datacenter > Cluster (pve1) > Join Information > Copy Information
-----------------------------------------------------------------

# ejemplo proxmox pve2
-----------------------------------------------------------------
Datacenter > Cluster (pve2) > Join Information >           
          > Information: <pego_informaci√≥n_copiada_anteriormente>
          > Password: <contrase√±a>
-----------------------------------------------------------------

# ejemplo proxmox pve1
# refresco p√°gina y ya deber√≠a ver el cluster creado son dos nodos pve1 y pve2
# para comprobar su funcionamiento y en caso opcional puedo migrar una m√°quina del pve1 al pve2 
Datacenter > Cluster (pve1) > VM (ejemplo: 101) > Sumary > Migrate       
          > Target Mode: pve2
          > Migrate

[üßÆ configuro quorum]::
https://www.youtube.com/watch?v=t5yvfnFvQrU
# en el caso de que tenga un n√∫mero impar de dispositivos me aseguro que el quorum funciona correctamente
Datacenter > HA > Quoruom:OK



=======[‚ùå CREAR UNA M√ÅQUINA VIRTUAL ]===============================================================
# buena pr√°ctica para mejor visualizaci√≥n
‚öôÔ∏è > Sort Key: Name

# creo una nueva m√°quina virtual
sobre el Node indicado > Create VM

# para crear una m√°quina virtual o contenedor es recomendable hacerlo desde:
local-lvm

# Ajustes al crear la m√°quina:
Create VM
------------------------------------------------------------------------------
+ General:
*********************************************************
Name: ~
_
+ OS:
*********************************************************
Storage: Synology (para producci√≥n) | local (para pruebas o desarrollo)
OS: (selecciono la √∫ltima versi√≥n de cada S.O)
_
+ System:
*********************************************************
_
+ Disk:
*********************************************************
Bus/Devicec: SATA | 0
Storage: Synology (para producci√≥n) | local (para pruebas o desarrollo)
Disk Size: 40 GiB (o los necesarios)
Cache: Write Back
         + Write back: mejor rendimiento, ideal para entornos donde la velocidad de I/O es prioritaria y hay respaldo el√©ctrico (UPS, almacenamiento con cach√© protegida). Riesgo de p√©rdida de datos en fallo.
         + Write through: mayor seguridad y consistencia, mejor para datos cr√≠ticos o producci√≥n donde la integridad es prioritaria, aunque con menor rendimiento.
  
[x] Avanced
~
[x] SSD emulation
_
+ CPU:
*********************************************************
# es recomendable poner un socket y varios cores
Sockets: 1 
Cores: 4
_
+ Memory:
*********************************************************
Memory (MiB): 2048 (o los que sean necesarios, en este caso 2GB | 4096 = 4GB) 
_
+ Network:
*********************************************************
Bridge: vmbr0
VLAN Tag: (el tag de vlan correspondiente no la vlan de red)
Model: 
------------------------------------------------------------------------------

